{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 3 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\">Long Short Term Memory (LSTM) for Language Modeling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Student 1:</b> Collura  \n",
    "<b> Student 2:</b> Spano\n",
    " \n",
    " \n",
    "In this Lab Session,  you will build and train a Recurrent Neural Network, based on Long Short-Term Memory (LSTM) units for next word prediction task. \n",
    "\n",
    "Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by June 9th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab3]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train a LSTM to predict the next word using a sample short story. The LSTM will learn to predict the next item of a sentence from the 3 previous items (given as input). Ponctuation marks are considered as dictionnary items so they can be predicted too. Figure 1 shows the LSTM and the process of next word prediction. \n",
    "\n",
    "<img src=\"lstm.png\" height=\"370\" width=\"370\"> \n",
    "\n",
    "\n",
    "Each word (and ponctuation) from text sentences is encoded by a unique integer. The integer value corresponds to the index of the corresponding word (or punctuation mark) in the dictionnary. The network output is a one-hot-vector indicating the index of the predicted word in the reversed dictionnary (Section 1.2). For example if the prediction is 86, the predicted word will be \"company\". \n",
    "\n",
    "\n",
    "\n",
    "You will use a sample short story from Aesopâ€™s Fables (http://www.taleswithmorals.com/) to train your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" face=\"verdana\" > <i> \"There was once a young Shepherd Boy who tended his sheep at the foot of a mountain near a dark forest.\n",
    "\n",
    "It was rather lonely for him all day, so he thought upon a plan by which he could get a little company and some excitement.\n",
    "He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.\n",
    "This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n",
    "But shortly after this a Wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out \"Wolf, Wolf,\" still louder than before.\n",
    "But this time the villagers, who had been fooled twice before, thought the boy was again deceiving them, and nobody stirred to come to his help.\n",
    "So the Wolf made a good meal off the boy's flock, and when the boy complained, the wise man of the village said:\n",
    "\"A liar will not be believed, even when he speaks the truth.\"  \"</i> </font>.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the necessary libraries and resetting the default computational graph. For more details about the rnn packages, we suggest you to take a look at https://www.tensorflow.org/api_guides/python/contrib.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections # used to build the dictionary\n",
    "import random\n",
    "import time\n",
    "import pickle # may be used to save your model \n",
    "import matplotlib.pyplot as plt\n",
    "#Import Tensorflow and rnn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn  \n",
    "\n",
    "# Target log path\n",
    "logs_path = 'lstm_words'\n",
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-word prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1: Data  preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split the text of our story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there' 'was' 'once' 'a' 'young' 'shepherd' 'boy' 'who' 'tended' 'his'\n",
      " 'sheep' 'at' 'the' 'foot' 'of' 'a' 'mountain' 'near' 'a' 'dark' 'forest'\n",
      " '.' 'it' 'was' 'rather' 'lonely' 'for' 'him' 'all' 'day' ',' 'so' 'he'\n",
      " 'thought' 'upon' 'a' 'plan' 'by' 'which' 'he' 'could' 'get' 'a' 'little'\n",
      " 'company' 'and' 'some' 'excitement' '.' 'he' 'rushed' 'down' 'towards'\n",
      " 'the' 'village' 'calling' 'out' 'wolf' ',' 'wolf' ',' 'and' 'the'\n",
      " 'villagers' 'came' 'out' 'to' 'meet' 'him' ',' 'and' 'some' 'of' 'them'\n",
      " 'stopped' 'with' 'him' 'for' 'a' 'considerable' 'time' '.' 'this'\n",
      " 'pleased' 'the' 'boy' 'so' 'much' 'that' 'a' 'few' 'days' 'afterwards'\n",
      " 'he' 'tried' 'the' 'same' 'trick' ',' 'and' 'again' 'the' 'villagers'\n",
      " 'came' 'to' 'his' 'help' '.' 'but' 'shortly' 'after' 'this' 'a' 'wolf'\n",
      " 'actually' 'did' 'come' 'out' 'from' 'the' 'forest' ',' 'and' 'began' 'to'\n",
      " 'worry' 'the' 'sheep,' 'and' 'the' 'boy' 'of' 'course' 'cried' 'out'\n",
      " 'wolf' ',' 'wolf' ',' 'still' 'louder' 'than' 'before' '.' 'but' 'this'\n",
      " 'time' 'the' 'villagers' ',' 'who' 'had' 'been' 'fooled' 'twice' 'before'\n",
      " ',' 'thought' 'the' 'boy' 'was' 'again' 'deceiving' 'them' ',' 'and'\n",
      " 'nobody' 'stirred' 'to' 'come' 'to' 'his' 'help' '.' 'so' 'the' 'wolf'\n",
      " 'made' 'a' 'good' 'meal' 'off' 'the' \"boy's\" 'flock' ',' 'and' 'when'\n",
      " 'the' 'boy' 'complained' ',' 'the' 'wise' 'man' 'of' 'the' 'village'\n",
      " 'said' ':' 'a' 'liar' 'will' 'not' 'be' 'believed' ',' 'even' 'when' 'he'\n",
      " 'speaks' 'the' 'truth' '.']\n",
      "Loaded training data...\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip().lower() for x in data]\n",
    "    data = [data[i].split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "#Run the cell \n",
    "train_file ='data/story.txt'\n",
    "train_data = load_data(train_file)\n",
    "print(\"Loaded training data...\")\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Symbols encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM input's can only be numbers. A way to convert words (symbols or any items) to numbers is to assign a unique integer to each word. This process is often based on frequency of occurrence for efficient coding purpose.\n",
    "\n",
    "Here, we define a function to build an indexed word dictionary (word->number). The \"build_vocabulary\" function builds both:\n",
    "\n",
    "- Dictionary : used for encoding words to numbers for the LSTM inputs \n",
    "- Reverted dictionnary : used for decoding the outputs of the LSTM into words (and punctuation).\n",
    "\n",
    "For example, in the story above, we have **113** individual words. The \"build_vocabulary\" function builds a dictionary with the following entries ['the': 0], [',': 1], ['company': 85],...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dic= dict()\n",
    "    for word, _ in count:\n",
    "        dic[word] = len(dic)\n",
    "    reverse_dic = dict(zip(dic.values(), dic.keys()))\n",
    "    return dic, reverse_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to display the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size (Vocabulary size) =  113\n",
      "\n",
      "\n",
      "Dictionary : \n",
      "\n",
      "{'was': 11, 'that': 101, 'had': 32, 'help': 17, 'afterwards': 33, 'when': 18, 'all': 34, 'again': 25, 'good': 36, 'stirred': 38, 'young': 39, 'forest': 19, 'much': 40, 'some': 27, 'foot': 42, 'sheep,': 43, 'course': 93, 'near': 45, 'shortly': 46, 'meet': 95, 'rather': 47, 'village': 20, ':': 48, 'them': 21, 'for': 22, 'so': 12, 'plan': 49, 'considerable': 50, 'truth': 51, 'there': 52, 'not': 53, 'twice': 54, 'day': 55, 'time': 23, 'meal': 56, 'at': 57, 'fooled': 58, 'began': 72, 'speaks': 59, 'louder': 60, 'excitement': 61, 'after': 62, 'once': 63, ',': 1, 'sheep': 64, 'which': 65, 'rushed': 66, 'deceiving': 67, 'and': 3, 'dark': 69, 'who': 28, 'actually': 73, 'little': 70, 'few': 71, 'shepherd': 35, 'before': 24, 'out': 9, 'made': 37, 'wise': 74, 'mountain': 83, 'nobody': 76, 'of': 10, 'him': 14, 'down': 77, 'upon': 78, 'wolf': 5, 'towards': 79, 'flock': 80, 'get': 82, 'thought': 26, 'liar': 75, 'villagers': 15, 'pleased': 84, 'tried': 85, 'cried': 86, 'than': 87, 'worry': 88, 'will': 81, 'even': 89, 'tended': 90, 'could': 91, 'it': 92, 'stopped': 44, 'same': 94, 'this': 13, 'days': 96, 'did': 41, 'off': 97, 'calling': 98, 'with': 99, 'he': 6, 'be': 100, 'a': 2, 'his': 16, 'boy': 7, '.': 4, 'come': 29, 'from': 102, 'by': 103, 'man': 104, 'believed': 105, 'complained': 106, 'to': 8, 'lonely': 107, 'trick': 111, 'still': 108, 'but': 30, 'company': 68, 'been': 109, 'said': 110, 'the': 0, \"boy's\": 112, 'came': 31}\n",
      "\n",
      "\n",
      "Reverted Dictionary : \n",
      "\n",
      "{0: 'the', 1: ',', 2: 'a', 3: 'and', 4: '.', 5: 'wolf', 6: 'he', 7: 'boy', 8: 'to', 9: 'out', 10: 'of', 11: 'was', 12: 'so', 13: 'this', 14: 'him', 15: 'villagers', 16: 'his', 17: 'help', 18: 'when', 19: 'forest', 20: 'village', 21: 'them', 22: 'for', 23: 'time', 24: 'before', 25: 'again', 26: 'thought', 27: 'some', 28: 'who', 29: 'come', 30: 'but', 31: 'came', 32: 'had', 33: 'afterwards', 34: 'all', 35: 'shepherd', 36: 'good', 37: 'made', 38: 'stirred', 39: 'young', 40: 'much', 41: 'did', 42: 'foot', 43: 'sheep,', 44: 'stopped', 45: 'near', 46: 'shortly', 47: 'rather', 48: ':', 49: 'plan', 50: 'considerable', 51: 'truth', 52: 'there', 53: 'not', 54: 'twice', 55: 'day', 56: 'meal', 57: 'at', 58: 'fooled', 59: 'speaks', 60: 'louder', 61: 'excitement', 62: 'after', 63: 'once', 64: 'sheep', 65: 'which', 66: 'rushed', 67: 'deceiving', 68: 'company', 69: 'dark', 70: 'little', 71: 'few', 72: 'began', 73: 'actually', 74: 'wise', 75: 'liar', 76: 'nobody', 77: 'down', 78: 'upon', 79: 'towards', 80: 'flock', 81: 'will', 82: 'get', 83: 'mountain', 84: 'pleased', 85: 'tried', 86: 'cried', 87: 'than', 88: 'worry', 89: 'even', 90: 'tended', 91: 'could', 92: 'it', 93: 'course', 94: 'same', 95: 'meet', 96: 'days', 97: 'off', 98: 'calling', 99: 'with', 100: 'be', 101: 'that', 102: 'from', 103: 'by', 104: 'man', 105: 'believed', 106: 'complained', 107: 'lonely', 108: 'still', 109: 'been', 110: 'said', 111: 'trick', 112: \"boy's\"}\n"
     ]
    }
   ],
   "source": [
    "dictionary, reverse_dictionary = build_vocabulary(train_data)\n",
    "vocabulary_size= len(dictionary) \n",
    "print(\"Dictionary size (Vocabulary size) = \", vocabulary_size)\n",
    "print(\"\\n\")\n",
    "print(\"Dictionary : \\n\")\n",
    "print(dictionary)\n",
    "print(\"\\n\")\n",
    "print(\"Reverted Dictionary : \\n\" )\n",
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : LSTM Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have defined how the data will be modeled, you are now to develop an LSTM model to predict the word of following a sequence of 3 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a 2-layers LSTM model.  \n",
    "\n",
    "For this use the following classes from the tensorflow.contrib library:\n",
    "\n",
    "- rnn.BasicLSTMCell(number of hidden units) \n",
    "- rnn.static_rnn(rnn_cell, data, dtype=tf.float32)\n",
    "- rnn.MultiRNNCell(,)\n",
    "\n",
    "\n",
    "You may need some tensorflow functions (https://www.tensorflow.org/api_docs/python/tf/) :\n",
    "- tf.split\n",
    "- tf.reshape \n",
    "- ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(x, w, b, n_hidden=265, n_input=3, n_layers=2):\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x, n_input, 1)\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden) for _ in range(n_layers)])\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], w['out']) + b['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "model_path = 'lstm_model/n%d' % n_input\n",
    "\n",
    "# For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "# LSTM  weights and biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size]))}\n",
    "\n",
    "# build the model\n",
    "pred = lstm_model(x, weights, biases, n_hidden=n_hidden, n_input=n_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Loss/Cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Cross Entropy loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)  # use RMSProp Optimizer\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We give you here the Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run the cell\n",
    "def test(sentence, session, verbose=False):\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != n_input:\n",
    "        print(\"sentence length should be equal to\", n_input, \"!\")\n",
    "    try:\n",
    "        symbols_inputs = [dictionary[str(words[i - n_input])] for i in range(n_input)]\n",
    "        keys = np.reshape(np.array(symbols_inputs), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        words.append(reverse_dictionary[onehot_pred_index])\n",
    "        sentence = \" \".join(words)\n",
    "        if verbose:\n",
    "            print(sentence)\n",
    "        return reverse_dictionary[onehot_pred_index]\n",
    "    except:\n",
    "        print(\" \".join([\"Word\", words[i - n_input], \"not in dictionary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : LSTM Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Training process, at each epoch, 3 words are taken from the training data, encoded to integer to form the input vector. The training labels are one-hot vector encoding the word that comes after the 3 inputs words. Display the loss and the training accuracy every 1000 iteration. Save the model at the end of training in the **lstm_model** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Average Loss= 4.395913, Average Accuracy= 7.10%\n",
      "['village', 'said', ':'] - [a] vs [the]\n",
      "Iter= 2000, Average Loss= 3.757541, Average Accuracy= 12.90%\n",
      "[',', 'and', 'when'] - [the] vs [.]\n",
      "Iter= 3000, Average Loss= 2.944651, Average Accuracy= 24.50%\n",
      "['flock', ',', 'and'] - [when] vs [when]\n",
      "Iter= 4000, Average Loss= 2.781739, Average Accuracy= 33.90%\n",
      "['the', 'village', 'said'] - [:] vs [,]\n",
      "Iter= 5000, Average Loss= 2.740451, Average Accuracy= 33.10%\n",
      "['young', 'shepherd', 'boy'] - [who] vs [who]\n",
      "Iter= 6000, Average Loss= 2.146430, Average Accuracy= 47.20%\n",
      "['near', 'a', 'dark'] - [forest] vs [the]\n",
      "Iter= 7000, Average Loss= 1.928356, Average Accuracy= 49.10%\n",
      "['rather', 'lonely', 'for'] - [him] vs [a]\n",
      "Iter= 8000, Average Loss= 1.673115, Average Accuracy= 55.50%\n",
      "['he', 'could', 'get'] - [a] vs [for]\n",
      "Iter= 9000, Average Loss= 1.572587, Average Accuracy= 58.10%\n",
      "['for', 'him', 'all'] - [day] vs [the]\n",
      "Iter= 10000, Average Loss= 1.191246, Average Accuracy= 68.20%\n",
      "['day', ',', 'so'] - [he] vs [he]\n",
      "Iter= 11000, Average Loss= 1.180018, Average Accuracy= 66.80%\n",
      "['it', 'was', 'rather'] - [lonely] vs [lonely]\n",
      "Iter= 12000, Average Loss= 1.035383, Average Accuracy= 72.60%\n",
      "['a', 'plan', 'by'] - [which] vs [which]\n",
      "Iter= 13000, Average Loss= 0.837094, Average Accuracy= 76.60%\n",
      "['plan', 'by', 'which'] - [he] vs [he]\n",
      "Iter= 14000, Average Loss= 0.762126, Average Accuracy= 78.90%\n",
      "['could', 'get', 'a'] - [little] vs [even]\n",
      "Iter= 15000, Average Loss= 0.701078, Average Accuracy= 80.80%\n",
      "['which', 'he', 'could'] - [get] vs [get]\n",
      "Iter= 16000, Average Loss= 0.543559, Average Accuracy= 85.60%\n",
      "['towards', 'the', 'village'] - [calling] vs [calling]\n",
      "Iter= 17000, Average Loss= 0.456444, Average Accuracy= 88.10%\n",
      "['to', 'meet', 'him'] - [,] vs [,]\n",
      "Iter= 18000, Average Loss= 0.498868, Average Accuracy= 85.60%\n",
      "['some', 'of', 'them'] - [stopped] vs [stopped]\n",
      "Iter= 19000, Average Loss= 0.453910, Average Accuracy= 87.60%\n",
      "['came', 'out', 'to'] - [meet] vs [meet]\n",
      "Iter= 20000, Average Loss= 0.399970, Average Accuracy= 89.10%\n",
      "['villagers', 'came', 'out'] - [to] vs [to]\n",
      "Iter= 21000, Average Loss= 0.400802, Average Accuracy= 87.40%\n",
      "['him', 'for', 'a'] - [considerable] vs [villagers]\n",
      "Iter= 22000, Average Loss= 0.348109, Average Accuracy= 89.70%\n",
      "['the', 'boy', 'so'] - [much] vs [much]\n",
      "Iter= 23000, Average Loss= 0.395350, Average Accuracy= 87.90%\n",
      "['same', 'trick', ','] - [and] vs [even]\n",
      "Iter= 24000, Average Loss= 0.332112, Average Accuracy= 90.40%\n",
      "['villagers', 'came', 'to'] - [his] vs [to]\n",
      "Iter= 25000, Average Loss= 0.361733, Average Accuracy= 90.50%\n",
      "['but', 'shortly', 'after'] - [this] vs [this]\n",
      "Iter= 26000, Average Loss= 0.312998, Average Accuracy= 91.40%\n",
      "['out', 'from', 'the'] - [forest] vs [forest]\n",
      "Iter= 27000, Average Loss= 0.376508, Average Accuracy= 87.10%\n",
      "['actually', 'did', 'come'] - [out] vs [out]\n",
      "Iter= 28000, Average Loss= 0.248364, Average Accuracy= 92.50%\n",
      "['shortly', 'after', 'this'] - [a] vs [a]\n",
      "Iter= 29000, Average Loss= 0.313399, Average Accuracy= 90.20%\n",
      "['trick', ',', 'and'] - [again] vs [again]\n",
      "Iter= 30000, Average Loss= 0.304598, Average Accuracy= 90.40%\n",
      "['villagers', 'came', 'to'] - [his] vs [his]\n",
      "Iter= 31000, Average Loss= 0.332861, Average Accuracy= 88.60%\n",
      "['villagers', 'came', 'to'] - [his] vs [to]\n",
      "Iter= 32000, Average Loss= 0.275027, Average Accuracy= 91.30%\n",
      "['to', 'his', 'help'] - [.] vs [.]\n",
      "Iter= 33000, Average Loss= 0.288363, Average Accuracy= 91.50%\n",
      "['tried', 'the', 'same'] - [trick] vs [trick]\n",
      "Iter= 34000, Average Loss= 0.301782, Average Accuracy= 89.90%\n",
      "[',', 'and', 'again'] - [the] vs [the]\n",
      "Iter= 35000, Average Loss= 0.271435, Average Accuracy= 91.40%\n",
      "['afterwards', 'he', 'tried'] - [the] vs [the]\n",
      "Iter= 36000, Average Loss= 0.286260, Average Accuracy= 90.20%\n",
      "['few', 'days', 'afterwards'] - [he] vs [.]\n",
      "Iter= 37000, Average Loss= 0.245458, Average Accuracy= 92.60%\n",
      "['so', 'much', 'that'] - [a] vs [a]\n",
      "Iter= 38000, Average Loss= 0.252979, Average Accuracy= 91.60%\n",
      "['same', 'trick', ','] - [and] vs [and]\n",
      "Iter= 39000, Average Loss= 0.251754, Average Accuracy= 90.90%\n",
      "[',', 'and', 'again'] - [the] vs [the]\n",
      "Iter= 40000, Average Loss= 0.269876, Average Accuracy= 91.20%\n",
      "['afterwards', 'he', 'tried'] - [the] vs [the]\n",
      "Iter= 41000, Average Loss= 0.284605, Average Accuracy= 91.20%\n",
      "['days', 'afterwards', 'he'] - [tried] vs [tried]\n",
      "Iter= 42000, Average Loss= 0.204882, Average Accuracy= 92.80%\n",
      "['come', 'out', 'from'] - [the] vs [the]\n",
      "Iter= 43000, Average Loss= 0.253962, Average Accuracy= 90.80%\n",
      "['forest', ',', 'and'] - [began] vs [began]\n",
      "Iter= 44000, Average Loss= 0.281755, Average Accuracy= 90.70%\n",
      "[',', 'and', 'began'] - [to] vs [stirred]\n",
      "Iter= 45000, Average Loss= 0.267580, Average Accuracy= 90.40%\n",
      "['to', 'worry', 'the'] - [sheep,] vs [same]\n",
      "Iter= 46000, Average Loss= 0.252657, Average Accuracy= 91.30%\n",
      "['forest', ',', 'and'] - [began] vs [began]\n",
      "Iter= 47000, Average Loss= 0.228221, Average Accuracy= 92.60%\n",
      "['shortly', 'after', 'this'] - [a] vs [a]\n",
      "Iter= 48000, Average Loss= 0.255558, Average Accuracy= 92.60%\n",
      "['.', 'but', 'shortly'] - [after] vs [after]\n",
      "Iter= 49000, Average Loss= 0.225237, Average Accuracy= 92.20%\n",
      "['help', '.', 'but'] - [shortly] vs [shortly]\n",
      "Iter= 50000, Average Loss= 0.230178, Average Accuracy= 92.30%\n",
      "['but', 'shortly', 'after'] - [this] vs [this]\n",
      "End Of training Finished!\n",
      "time:  63.46682286262512\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and oint your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < epochs:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    " \n",
    "    ##############################################\n",
    "    \n",
    "    print(\"End Of training Finished!\")\n",
    "    print(\"time: \",time.time() - start_time)\n",
    "    print(\"For tensorboard visualisation run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"and oint your web browser to the returned link\")\n",
    "    ##############################################\n",
    "    # save your model \n",
    "    ##############################################\n",
    "    model_saver.save(session, model_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4 : Test your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1. Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your model (using the model_saved variable given in the training session) and test the sentences :\n",
    "- 'get a little' \n",
    "- 'nobody tried to'\n",
    "- Try with other sentences using words from the story's vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a little days\n",
      "nobody tried to wolf\n",
      "\n",
      "\n",
      ". but shortly after\n",
      " Predicted word: after \n",
      " Word from text: after\n",
      "trick , and when\n",
      " Predicted word: when \n",
      " Word from text: again\n",
      "company and some excitement\n",
      " Predicted word: excitement \n",
      " Word from text: excitement\n",
      "been fooled twice before\n",
      " Predicted word: before \n",
      " Word from text: before\n",
      "when he speaks the\n",
      " Predicted word: the \n",
      " Word from text: the\n",
      "a little company not\n",
      " Predicted word: not \n",
      " Word from text: and\n",
      "all day , so\n",
      " Predicted word: so \n",
      " Word from text: so\n",
      "he could get a\n",
      " Predicted word: a \n",
      " Word from text: a\n",
      "to his help .\n",
      " Predicted word: . \n",
      " Word from text: .\n",
      "wolf , wolf ,\n",
      " Predicted word: , \n",
      " Word from text: ,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "\n",
    "    sentences = ['get a little', 'nobody tried to']\n",
    "    for sentence in sentences:\n",
    "        test(sentence, session, True)\n",
    "    \n",
    "    print('\\n')\n",
    "    sentences = [train_data[n:n + n_input + 1] for n in np.random.randint(0, len(train_data) - n_input - 1, 10)]\n",
    "    for sentence in sentences:\n",
    "        predicted = test(' '.join(sentence[:-1]), session, True)\n",
    "        print(' Predicted word:', predicted, '\\n Word from text:', sentence[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. More fun with the Fable Writer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the RNN/LSTM model learned in the previous question to create a\n",
    "new story/fable.\n",
    "For this you will choose 3 words from the dictionary which will start your\n",
    "story and initialize your network. Using those 3 words the RNN will generate\n",
    "the next word or the story. Using the last 3 words (the newly predicted one\n",
    "and the last 2 from the input) you will use the network to predict the 5\n",
    "word of the story.. and so on until your story is 5 sentence long. \n",
    "Make a point at the end of your story. \n",
    "To implement that, you will use the test function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wise man of wolf , wolf , still louder than before . but this time the considerable the of so much that a few days not be before . but this time the considerable the of so much that a few days not be before . but this time the considerable the of so much that a few days not be before . but this time the considerable the of so much that a few days not be before .\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "    n = np.random.randint(0, len(train_data) - n_input)\n",
    "    words = train_data[n:n + n_input].tolist()\n",
    "    while words.count('.') < 5 and len(words) < 1000:  # set an upper bound on iterations\n",
    "        sentence = ' '.join(words[-n_input:])\n",
    "        words.append(test(sentence, session))\n",
    "\n",
    "if words[-1] != '.':\n",
    "    words.append('.')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "As we can see, we get stuck in a loop. This is due to the fact that the window of words we consider for the prediction is too small: we only look at the previous 3 words to get the next one, so for example, every time we start from \"a few days\" we will always predict \"not\", every time we start from \"few days not\", we will get \"he\" and so on. We will see if increasing the window of input words to 5 will solve the problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Play with number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The number of input in our example is 3, see what happens when you use other number (1 and 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Number of inputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 5\n",
    "model_path = 'lstm_model/n%d' % n_input\n",
    "\n",
    "# For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "# LSTM  weights and biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size]))}\n",
    "\n",
    "# build the model\n",
    "pred = lstm_model(x, weights, biases, n_hidden=n_hidden, n_input=n_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Loss/Cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Cross Entropy loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)  # use RMSProp Optimizer\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Average Loss= 4.478766, Average Accuracy= 5.30%\n",
      "['the', 'wolf', 'made', 'a', 'good'] - [meal] vs [,]\n",
      "Iter= 2000, Average Loss= 3.535477, Average Accuracy= 18.50%\n",
      "['boy', 'of', 'course', 'cried', 'out'] - [wolf] vs [,]\n",
      "Iter= 3000, Average Loss= 2.841516, Average Accuracy= 31.50%\n",
      "['time', '.', 'this', 'pleased', 'the'] - [boy] vs [boy]\n",
      "Iter= 4000, Average Loss= 2.312626, Average Accuracy= 44.10%\n",
      "[',', 'wolf', ',', 'and', 'the'] - [villagers] vs [and]\n",
      "Iter= 5000, Average Loss= 1.396652, Average Accuracy= 64.20%\n",
      "['there', 'was', 'once', 'a', 'young'] - [shepherd] vs [shepherd]\n",
      "Iter= 6000, Average Loss= 0.719507, Average Accuracy= 81.80%\n",
      "[',', 'the', 'wise', 'man', 'of'] - [the] vs [the]\n",
      "Iter= 7000, Average Loss= 0.460446, Average Accuracy= 88.60%\n",
      "['the', 'boy', 'was', 'again', 'deceiving'] - [them] vs [them]\n",
      "Iter= 8000, Average Loss= 0.333728, Average Accuracy= 91.00%\n",
      "['same', 'trick', ',', 'and', 'again'] - [the] vs [the]\n",
      "Iter= 9000, Average Loss= 0.154757, Average Accuracy= 96.10%\n",
      "['thought', 'upon', 'a', 'plan', 'by'] - [which] vs [which]\n",
      "Iter= 10000, Average Loss= 0.163079, Average Accuracy= 95.50%\n",
      "['a', 'liar', 'will', 'not', 'be'] - [believed] vs [believed]\n",
      "Iter= 11000, Average Loss= 0.123041, Average Accuracy= 96.80%\n",
      "['twice', 'before', ',', 'thought', 'the'] - [boy] vs [boy]\n",
      "Iter= 12000, Average Loss= 0.096775, Average Accuracy= 96.80%\n",
      "[',', 'and', 'again', 'the', 'villagers'] - [came] vs [came]\n",
      "Iter= 13000, Average Loss= 0.102873, Average Accuracy= 97.20%\n",
      "['which', 'he', 'could', 'get', 'a'] - [little] vs [little]\n",
      "Iter= 14000, Average Loss= 0.063075, Average Accuracy= 97.80%\n",
      "['boy', 'who', 'tended', 'his', 'sheep'] - [at] vs [at]\n",
      "Iter= 15000, Average Loss= 0.058523, Average Accuracy= 98.30%\n",
      "['wolf', 'made', 'a', 'good', 'meal'] - [off] vs [off]\n",
      "Iter= 16000, Average Loss= 0.038566, Average Accuracy= 99.00%\n",
      "['stirred', 'to', 'come', 'to', 'his'] - [help] vs [help]\n",
      "Iter= 17000, Average Loss= 0.074246, Average Accuracy= 97.90%\n",
      "['the', 'sheep,', 'and', 'the', 'boy'] - [of] vs [of]\n",
      "Iter= 18000, Average Loss= 0.047590, Average Accuracy= 98.60%\n",
      "['a', 'considerable', 'time', '.', 'this'] - [pleased] vs [pleased]\n",
      "Iter= 19000, Average Loss= 0.048984, Average Accuracy= 98.60%\n",
      "['by', 'which', 'he', 'could', 'get'] - [a] vs [a]\n",
      "Iter= 20000, Average Loss= 0.051284, Average Accuracy= 98.50%\n",
      "['a', 'young', 'shepherd', 'boy', 'who'] - [tended] vs [tended]\n",
      "Iter= 21000, Average Loss= 0.057004, Average Accuracy= 97.80%\n",
      "['.', 'so', 'the', 'wolf', 'made'] - [a] vs [a]\n",
      "Iter= 22000, Average Loss= 0.020620, Average Accuracy= 99.10%\n",
      "['boy', 'of', 'course', 'cried', 'out'] - [wolf] vs [wolf]\n",
      "Iter= 23000, Average Loss= 0.030939, Average Accuracy= 98.70%\n",
      "['trick', ',', 'and', 'again', 'the'] - [villagers] vs [villagers]\n",
      "Iter= 24000, Average Loss= 0.033684, Average Accuracy= 98.50%\n",
      "['to', 'meet', 'him', ',', 'and'] - [some] vs [some]\n",
      "Iter= 25000, Average Loss= 0.035001, Average Accuracy= 98.60%\n",
      "['near', 'a', 'dark', 'forest', '.'] - [it] vs [it]\n",
      "Iter= 26000, Average Loss= 0.015168, Average Accuracy= 99.40%\n",
      "['the', 'village', 'said', ':', 'a'] - [liar] vs [liar]\n",
      "Iter= 27000, Average Loss= 0.031418, Average Accuracy= 98.60%\n",
      "['to', 'come', 'to', 'his', 'help'] - [.] vs [.]\n",
      "Iter= 28000, Average Loss= 0.029878, Average Accuracy= 98.80%\n",
      "['than', 'before', '.', 'but', 'this'] - [time] vs [time]\n",
      "Iter= 29000, Average Loss= 0.020606, Average Accuracy= 99.20%\n",
      "['villagers', 'came', 'to', 'his', 'help'] - [.] vs [.]\n",
      "Iter= 30000, Average Loss= 0.022891, Average Accuracy= 99.00%\n",
      "['of', 'them', 'stopped', 'with', 'him'] - [for] vs [for]\n",
      "Iter= 31000, Average Loss= 0.024491, Average Accuracy= 99.10%\n",
      "['could', 'get', 'a', 'little', 'company'] - [and] vs [and]\n",
      "Iter= 32000, Average Loss= 0.024257, Average Accuracy= 99.20%\n",
      "['boy', 'who', 'tended', 'his', 'sheep'] - [at] vs [at]\n",
      "Iter= 33000, Average Loss= 0.022765, Average Accuracy= 98.90%\n",
      "['again', 'deceiving', 'them', ',', 'and'] - [nobody] vs [nobody]\n",
      "Iter= 34000, Average Loss= 0.019818, Average Accuracy= 99.30%\n",
      "['forest', ',', 'and', 'began', 'to'] - [worry] vs [worry]\n",
      "Iter= 35000, Average Loss= 0.019838, Average Accuracy= 99.30%\n",
      "['few', 'days', 'afterwards', 'he', 'tried'] - [the] vs [the]\n",
      "Iter= 36000, Average Loss= 0.011546, Average Accuracy= 99.60%\n",
      "['to', 'meet', 'him', ',', 'and'] - [some] vs [some]\n",
      "Iter= 37000, Average Loss= 0.022299, Average Accuracy= 99.20%\n",
      "['company', 'and', 'some', 'excitement', '.'] - [he] vs [he]\n",
      "Iter= 38000, Average Loss= 0.026485, Average Accuracy= 98.70%\n",
      "['shepherd', 'boy', 'who', 'tended', 'his'] - [sheep] vs [sheep]\n",
      "Iter= 39000, Average Loss= 0.021995, Average Accuracy= 98.90%\n",
      "['his', 'help', '.', 'so', 'the'] - [wolf] vs [wolf]\n",
      "Iter= 40000, Average Loss= 0.016827, Average Accuracy= 99.30%\n",
      "['a', 'wolf', 'actually', 'did', 'come'] - [out] vs [out]\n",
      "Iter= 41000, Average Loss= 0.023304, Average Accuracy= 98.90%\n",
      "['down', 'towards', 'the', 'village', 'calling'] - [out] vs [out]\n",
      "Iter= 42000, Average Loss= 0.019125, Average Accuracy= 99.10%\n",
      "['it', 'was', 'rather', 'lonely', 'for'] - [him] vs [him]\n",
      "Iter= 43000, Average Loss= 0.011421, Average Accuracy= 99.50%\n",
      "['complained', ',', 'the', 'wise', 'man'] - [of] vs [of]\n",
      "Iter= 44000, Average Loss= 0.016053, Average Accuracy= 99.30%\n",
      "['course', 'cried', 'out', 'wolf', ','] - [wolf] vs [wolf]\n",
      "Iter= 45000, Average Loss= 0.023626, Average Accuracy= 98.90%\n",
      "['villagers', 'came', 'to', 'his', 'help'] - [.] vs [.]\n",
      "Iter= 46000, Average Loss= 0.023847, Average Accuracy= 98.90%\n",
      "['the', 'village', 'calling', 'out', 'wolf'] - [,] vs [,]\n",
      "Iter= 47000, Average Loss= 0.011247, Average Accuracy= 99.40%\n",
      "['near', 'a', 'dark', 'forest', '.'] - [it] vs [it]\n",
      "Iter= 48000, Average Loss= 0.020420, Average Accuracy= 99.10%\n",
      "['a', 'good', 'meal', 'off', 'the'] - [boy's] vs [boy's]\n",
      "Iter= 49000, Average Loss= 0.022316, Average Accuracy= 98.90%\n",
      "['course', 'cried', 'out', 'wolf', ','] - [wolf] vs [wolf]\n",
      "Iter= 50000, Average Loss= 0.015489, Average Accuracy= 99.30%\n",
      "['the', 'forest', ',', 'and', 'began'] - [to] vs [to]\n",
      "End Of training Finished!\n",
      "time:  96.46503400802612\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and oint your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < epochs:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    " \n",
    "    ##############################################\n",
    "    \n",
    "    print(\"End Of training Finished!\")\n",
    "    print(\"time: \",time.time() - start_time)\n",
    "    print(\"For tensorboard visualisation run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"and oint your web browser to the returned link\")\n",
    "    ##############################################\n",
    "    # save your model \n",
    "    ##############################################\n",
    "    model_saver.save(session, model_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complained , the wise man of\n",
      " Predicted word: of \n",
      " Word from text: of\n",
      "for him all day , so\n",
      " Predicted word: so \n",
      " Word from text: so\n",
      "the boy of course cried out\n",
      " Predicted word: out \n",
      " Word from text: out\n",
      "the boy so much that a\n",
      " Predicted word: a \n",
      " Word from text: a\n",
      "but this time the villagers ,\n",
      " Predicted word: , \n",
      " Word from text: ,\n",
      "thought the boy was again deceiving\n",
      " Predicted word: deceiving \n",
      " Word from text: deceiving\n",
      "but shortly after this a wolf\n",
      " Predicted word: wolf \n",
      " Word from text: wolf\n",
      "tended his sheep at the foot\n",
      " Predicted word: foot \n",
      " Word from text: foot\n",
      "forest . it was rather lonely\n",
      " Predicted word: lonely \n",
      " Word from text: lonely\n",
      ", wolf , still louder than\n",
      " Predicted word: than \n",
      " Word from text: than\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "\n",
    "    sentences = [train_data[n:n + n_input + 1] for n in np.random.randint(0, len(train_data) - n_input - 1, 10)]\n",
    "    for sentence in sentences:\n",
    "        predicted = test(' '.join(sentence[:-1]), session, True)\n",
    "        print(' Predicted word:', predicted, '\\n Word from text:', sentence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it was rather lonely for him all day , so he thought upon a plan by which he could get a little company and some excitement . he rushed down towards the village calling out wolf , wolf , and the villagers came out to meet him , and some of them stopped with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the villagers came to his help . but shortly after this a wolf actually did come out from the forest , and began to worry the sheep, and the boy of course cried out wolf , wolf , and the villagers came out to meet him , and some of them stopped with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the villagers came to his help .\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "    n = np.random.randint(0, len(train_data) - n_input)\n",
    "    words = train_data[n:n + n_input].tolist()\n",
    "    while words.count('.') < 5 and len(words) < 1000:  # set an upper bound on iterations\n",
    "        sentence = ' '.join(words[-n_input:])\n",
    "        words.append(test(sentence, session))\n",
    "\n",
    "if words[-1] != '.':\n",
    "    words.append('.')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" face=\"verdana\" > <i> \"There was once a young Shepherd Boy who tended his sheep at the foot of a mountain near a dark forest.\n",
    "\n",
    "It was rather lonely for him all day, so he thought upon a plan by which he could get a little company and some excitement.\n",
    "He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.\n",
    "This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n",
    "But shortly after this a Wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out \"Wolf, Wolf,\" still louder than before.\n",
    "But this time the villagers, who had been fooled twice before, thought the boy was again deceiving them, and nobody stirred to come to his help.\n",
    "So the Wolf made a good meal off the boy's flock, and when the boy complained, the wise man of the village said:\n",
    "\"A liar will not be believed, even when he speaks the truth.\"  \"</i> </font>.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "As we can see the text we obtain is exactly the same as the original. We believe this is due to overfitting on the training data: in fact, the window of words we train on is too big, and we end up guessing all the right words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Number of inputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 1\n",
    "model_path = 'lstm_model/n%d' % n_input\n",
    "\n",
    "# For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "# LSTM  weights and biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size]))}\n",
    "\n",
    "# build the model\n",
    "pred = lstm_model(x, weights, biases, n_hidden=n_hidden, n_input=n_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Loss/Cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Cross Entropy loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)  # use RMSProp Optimizer\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Average Loss= 4.631672, Average Accuracy= 4.10%\n",
      "['much'] - [that] vs [.]\n",
      "Iter= 2000, Average Loss= 4.270354, Average Accuracy= 6.70%\n",
      "['the'] - [wolf] vs [and]\n",
      "Iter= 3000, Average Loss= 4.124525, Average Accuracy= 8.60%\n",
      "['village'] - [calling] vs [a]\n",
      "Iter= 4000, Average Loss= 4.044169, Average Accuracy= 10.00%\n",
      "['than'] - [before] vs [the]\n",
      "Iter= 5000, Average Loss= 3.958994, Average Accuracy= 11.30%\n",
      "['of'] - [a] vs [,]\n",
      "Iter= 6000, Average Loss= 4.038328, Average Accuracy= 11.20%\n",
      "['villagers'] - [came] vs [.]\n",
      "Iter= 7000, Average Loss= 3.869531, Average Accuracy= 11.60%\n",
      "['flock'] - [,] vs [the]\n",
      "Iter= 8000, Average Loss= 4.091427, Average Accuracy= 11.00%\n",
      "['out'] - [wolf] vs [a]\n",
      "Iter= 9000, Average Loss= 3.683072, Average Accuracy= 13.70%\n",
      "[','] - [still] vs [wolf]\n",
      "Iter= 10000, Average Loss= 3.862956, Average Accuracy= 12.90%\n",
      "['his'] - [sheep] vs [.]\n",
      "Iter= 11000, Average Loss= 3.947990, Average Accuracy= 12.50%\n",
      "['afterwards'] - [he] vs [out]\n",
      "Iter= 12000, Average Loss= 3.659537, Average Accuracy= 13.30%\n",
      "['wolf'] - [made] vs [his]\n",
      "Iter= 13000, Average Loss= 3.908325, Average Accuracy= 12.70%\n",
      "['he'] - [rushed] vs [could]\n",
      "Iter= 14000, Average Loss= 3.765269, Average Accuracy= 12.60%\n",
      "[','] - [still] vs [wolf]\n",
      "Iter= 15000, Average Loss= 3.806042, Average Accuracy= 11.70%\n",
      "['the'] - [foot] vs [boy]\n",
      "Iter= 16000, Average Loss= 3.806657, Average Accuracy= 10.70%\n",
      "['tried'] - [the] vs [.]\n",
      "Iter= 17000, Average Loss= 3.796254, Average Accuracy= 10.90%\n",
      "['good'] - [meal] vs [to]\n",
      "Iter= 18000, Average Loss= 3.798835, Average Accuracy= 10.10%\n",
      "['out'] - [wolf] vs [he]\n",
      "Iter= 19000, Average Loss= 3.708694, Average Accuracy= 12.00%\n",
      "['.'] - [but] vs [,]\n",
      "Iter= 20000, Average Loss= 3.716402, Average Accuracy= 12.70%\n",
      "['mountain'] - [near] vs [the]\n",
      "Iter= 21000, Average Loss= 3.592539, Average Accuracy= 12.30%\n",
      "['villagers'] - [came] vs [.]\n",
      "Iter= 22000, Average Loss= 3.880419, Average Accuracy= 11.70%\n",
      "['the'] - [boy] vs [boy]\n",
      "Iter= 23000, Average Loss= 3.603487, Average Accuracy= 12.50%\n",
      "['the'] - [villagers] vs [boy]\n",
      "Iter= 24000, Average Loss= 3.334717, Average Accuracy= 14.60%\n",
      "['villagers'] - [,] vs [.]\n",
      "Iter= 25000, Average Loss= 4.055398, Average Accuracy= 11.40%\n",
      "['all'] - [day] vs [a]\n",
      "Iter= 26000, Average Loss= 3.007195, Average Accuracy= 15.60%\n",
      "['after'] - [this] vs [the]\n",
      "Iter= 27000, Average Loss= 3.241652, Average Accuracy= 13.80%\n",
      "['the'] - [wise] vs [boy]\n",
      "Iter= 28000, Average Loss= 3.455157, Average Accuracy= 13.20%\n",
      "['to'] - [meet] vs [wolf]\n",
      "Iter= 29000, Average Loss= 3.852760, Average Accuracy= 11.90%\n",
      "[','] - [thought] vs [and]\n",
      "Iter= 30000, Average Loss= 3.691090, Average Accuracy= 11.80%\n",
      "['all'] - [day] vs [a]\n",
      "Iter= 31000, Average Loss= 3.561308, Average Accuracy= 12.50%\n",
      "['from'] - [the] vs [the]\n",
      "Iter= 32000, Average Loss= 3.412530, Average Accuracy= 12.90%\n",
      "['be'] - [believed] vs [:]\n",
      "Iter= 33000, Average Loss= 3.155835, Average Accuracy= 14.00%\n",
      "['him'] - [for] vs [,]\n",
      "Iter= 34000, Average Loss= 3.433062, Average Accuracy= 13.40%\n",
      "['was'] - [again] vs [the]\n",
      "Iter= 35000, Average Loss= 3.725866, Average Accuracy= 13.20%\n",
      "['upon'] - [a] vs [a]\n",
      "Iter= 36000, Average Loss= 3.842331, Average Accuracy= 12.10%\n",
      "['and'] - [began] vs [little]\n",
      "Iter= 37000, Average Loss= 3.165194, Average Accuracy= 14.30%\n",
      "[','] - [even] vs [and]\n",
      "Iter= 38000, Average Loss= 4.036096, Average Accuracy= 13.00%\n",
      "['.'] - [this] vs [,]\n",
      "Iter= 39000, Average Loss= 3.417271, Average Accuracy= 13.80%\n",
      "['to'] - [come] vs [his]\n",
      "Iter= 40000, Average Loss= 3.660294, Average Accuracy= 12.90%\n",
      "['could'] - [get] vs [a]\n",
      "Iter= 41000, Average Loss= 3.745808, Average Accuracy= 12.70%\n",
      "['to'] - [worry] vs [,]\n",
      "Iter= 42000, Average Loss= 3.567561, Average Accuracy= 12.70%\n",
      "['speaks'] - [the] vs [off]\n",
      "Iter= 43000, Average Loss= 3.559134, Average Accuracy= 12.00%\n",
      "['time'] - [.] vs [.]\n",
      "Iter= 44000, Average Loss= 3.711828, Average Accuracy= 10.70%\n",
      "['come'] - [to] vs [,]\n",
      "Iter= 45000, Average Loss= 3.312493, Average Accuracy= 12.20%\n",
      "['which'] - [he] vs [a]\n",
      "Iter= 46000, Average Loss= 3.602122, Average Accuracy= 12.20%\n",
      "['sheep,'] - [and] vs [to]\n",
      "Iter= 47000, Average Loss= 3.426642, Average Accuracy= 11.40%\n",
      "['speaks'] - [the] vs [,]\n",
      "Iter= 48000, Average Loss= 3.241038, Average Accuracy= 12.30%\n",
      "['.'] - [this] vs [the]\n",
      "Iter= 49000, Average Loss= 3.480690, Average Accuracy= 12.50%\n",
      "['to'] - [his] vs [come]\n",
      "Iter= 50000, Average Loss= 3.581636, Average Accuracy= 11.50%\n",
      "['a'] - [little] vs [good]\n",
      "End Of training Finished!\n",
      "time:  48.03979253768921\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and oint your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < epochs:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    " \n",
    "    ##############################################\n",
    "    \n",
    "    print(\"End Of training Finished!\")\n",
    "    print(\"time: \",time.time() - start_time)\n",
    "    print(\"For tensorboard visualisation run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"and oint your web browser to the returned link\")\n",
    "    ##############################################\n",
    "    # save your model \n",
    "    ##############################################\n",
    "    model_saver.save(session, model_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolf who\n",
      " Predicted word: who \n",
      " Word from text: actually\n",
      "and some\n",
      " Predicted word: some \n",
      " Word from text: some\n",
      "with was\n",
      " Predicted word: was \n",
      " Word from text: him\n",
      "his a\n",
      " Predicted word: a \n",
      " Word from text: help\n",
      "help a\n",
      " Predicted word: a \n",
      " Word from text: .\n",
      "calling was\n",
      " Predicted word: was \n",
      " Word from text: out\n",
      "come .\n",
      " Predicted word: . \n",
      " Word from text: out\n",
      ", and\n",
      " Predicted word: and \n",
      " Word from text: who\n",
      "thought a\n",
      " Predicted word: a \n",
      " Word from text: the\n",
      "his a\n",
      " Predicted word: a \n",
      " Word from text: help\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "\n",
    "    sentences = [train_data[n:n + n_input + 1] for n in np.random.randint(0, len(train_data) - n_input - 1, 10)]\n",
    "    for sentence in sentences:\n",
    "        predicted = test(' '.join(sentence[:-1]), session, True)\n",
    "        print(' Predicted word:', predicted, '\\n Word from text:', sentence[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "As expected given the low accuracy obtained in the training phase, we do not have good guesses on the next words in this case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy who . the boy who . the boy who . the boy who . the boy who .\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "    n = np.random.randint(0, len(train_data) - n_input)\n",
    "    words = train_data[n:n + n_input].tolist()\n",
    "    while words.count('.') < 5 and len(words) < 1000:  # set an upper bound on iterations\n",
    "        sentence = ' '.join(words[-n_input:])\n",
    "        words.append(test(sentence, session))\n",
    "\n",
    "if words[-1] != '.':\n",
    "    words.append('.')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "As happened above with input=3, here we are stuck in a loop and we keep printing the same sentences over again. This is due to the fact that the window is too small, and it predicts only based on the last word: every time it finds \",\" for example, it will always predict \"and\" as the following word. Same with \"and\" and \"the\" and so on.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Number of inputs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 4\n",
    "model_path = 'lstm_model/n%d' % n_input\n",
    "\n",
    "# For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "# LSTM  weights and biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size]))}\n",
    "\n",
    "# build the model\n",
    "pred = lstm_model(x, weights, biases, n_hidden=n_hidden, n_input=n_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Loss/Cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Cross Entropy loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)  # use RMSProp Optimizer\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Average Loss= 4.322022, Average Accuracy= 7.70%\n",
      "['believed', ',', 'even', 'when'] - [he] vs [man]\n",
      "Iter= 2000, Average Loss= 3.607812, Average Accuracy= 16.80%\n",
      "['said', ':', 'a', 'liar'] - [will] vs [will]\n",
      "Iter= 3000, Average Loss= 3.167056, Average Accuracy= 27.20%\n",
      "['his', 'help', '.', 'so'] - [the] vs [,]\n",
      "Iter= 4000, Average Loss= 2.415726, Average Accuracy= 42.60%\n",
      "['boy', 'was', 'again', 'deceiving'] - [them] vs [them]\n",
      "Iter= 5000, Average Loss= 2.113297, Average Accuracy= 46.80%\n",
      "['of', 'course', 'cried', 'out'] - [wolf] vs [wolf]\n",
      "Iter= 6000, Average Loss= 1.343372, Average Accuracy= 65.40%\n",
      "['.', 'but', 'shortly', 'after'] - [this] vs [,]\n",
      "Iter= 7000, Average Loss= 0.839098, Average Accuracy= 78.40%\n",
      "['villagers', 'came', 'to', 'his'] - [help] vs [stopped]\n",
      "Iter= 8000, Average Loss= 0.794565, Average Accuracy= 80.10%\n",
      "['that', 'a', 'few', 'days'] - [afterwards] vs [afterwards]\n",
      "Iter= 9000, Average Loss= 0.441421, Average Accuracy= 88.80%\n",
      "['a', 'few', 'days', 'afterwards'] - [he] vs [be]\n",
      "Iter= 10000, Average Loss= 0.433503, Average Accuracy= 90.00%\n",
      "['and', 'the', 'villagers', 'came'] - [out] vs [out]\n",
      "Iter= 11000, Average Loss= 0.281733, Average Accuracy= 93.80%\n",
      "['he', 'thought', 'upon', 'a'] - [plan] vs [plan]\n",
      "Iter= 12000, Average Loss= 0.274966, Average Accuracy= 92.20%\n",
      "['who', 'tended', 'his', 'sheep'] - [at] vs [at]\n",
      "Iter= 13000, Average Loss= 0.235878, Average Accuracy= 94.10%\n",
      "['will', 'not', 'be', 'believed'] - [,] vs [,]\n",
      "Iter= 14000, Average Loss= 0.168872, Average Accuracy= 95.00%\n",
      "['come', 'to', 'his', 'help'] - [.] vs [.]\n",
      "Iter= 15000, Average Loss= 0.126080, Average Accuracy= 96.40%\n",
      "['before', ',', 'thought', 'the'] - [boy] vs [boy]\n",
      "Iter= 16000, Average Loss= 0.119219, Average Accuracy= 96.80%\n",
      "['before', '.', 'but', 'this'] - [time] vs [time]\n",
      "Iter= 17000, Average Loss= 0.104220, Average Accuracy= 97.20%\n",
      "['out', 'from', 'the', 'forest'] - [,] vs [,]\n",
      "Iter= 18000, Average Loss= 0.088860, Average Accuracy= 97.40%\n",
      "['same', 'trick', ',', 'and'] - [again] vs [again]\n",
      "Iter= 19000, Average Loss= 0.108009, Average Accuracy= 97.50%\n",
      "['some', 'of', 'them', 'stopped'] - [with] vs [with]\n",
      "Iter= 20000, Average Loss= 0.086859, Average Accuracy= 97.70%\n",
      "['could', 'get', 'a', 'little'] - [company] vs [company]\n",
      "Iter= 21000, Average Loss= 0.102897, Average Accuracy= 97.10%\n",
      "['lonely', 'for', 'him', 'all'] - [day] vs [day]\n",
      "Iter= 22000, Average Loss= 0.093824, Average Accuracy= 97.00%\n",
      "['a', 'mountain', 'near', 'a'] - [dark] vs [dark]\n",
      "Iter= 23000, Average Loss= 0.075067, Average Accuracy= 97.80%\n",
      "['of', 'a', 'mountain', 'near'] - [a] vs [a]\n",
      "Iter= 24000, Average Loss= 0.047896, Average Accuracy= 98.70%\n",
      "['of', 'a', 'mountain', 'near'] - [a] vs [a]\n",
      "Iter= 25000, Average Loss= 0.096340, Average Accuracy= 97.70%\n",
      "[',', 'the', 'wise', 'man'] - [of] vs [of]\n",
      "Iter= 26000, Average Loss= 0.083499, Average Accuracy= 98.60%\n",
      "['come', 'to', 'his', 'help'] - [.] vs [.]\n",
      "Iter= 27000, Average Loss= 0.109129, Average Accuracy= 97.40%\n",
      "['who', 'had', 'been', 'fooled'] - [twice] vs [twice]\n",
      "Iter= 28000, Average Loss= 0.097061, Average Accuracy= 97.70%\n",
      "['boy', 'of', 'course', 'cried'] - [out] vs [out]\n",
      "Iter= 29000, Average Loss= 0.085984, Average Accuracy= 98.10%\n",
      "['boy', 'of', 'course', 'cried'] - [out] vs [out]\n",
      "Iter= 30000, Average Loss= 0.090609, Average Accuracy= 98.00%\n",
      "['wolf', 'actually', 'did', 'come'] - [out] vs [out]\n",
      "Iter= 31000, Average Loss= 0.115120, Average Accuracy= 97.30%\n",
      "['again', 'the', 'villagers', 'came'] - [to] vs [to]\n",
      "Iter= 32000, Average Loss= 0.092904, Average Accuracy= 97.40%\n",
      "['that', 'a', 'few', 'days'] - [afterwards] vs [afterwards]\n",
      "Iter= 33000, Average Loss= 0.097197, Average Accuracy= 98.20%\n",
      "['the', 'villagers', 'came', 'out'] - [to] vs [his]\n",
      "Iter= 34000, Average Loss= 0.058194, Average Accuracy= 98.80%\n",
      "['for', 'him', 'all', 'day'] - [,] vs [,]\n",
      "Iter= 35000, Average Loss= 0.103635, Average Accuracy= 98.00%\n",
      "['a', 'mountain', 'near', 'a'] - [dark] vs [dark]\n",
      "Iter= 36000, Average Loss= 0.098919, Average Accuracy= 97.90%\n",
      "['liar', 'will', 'not', 'be'] - [believed] vs [believed]\n",
      "Iter= 37000, Average Loss= 0.093902, Average Accuracy= 97.80%\n",
      "['complained', ',', 'the', 'wise'] - [man] vs [man]\n",
      "Iter= 38000, Average Loss= 0.084031, Average Accuracy= 98.10%\n",
      "['stirred', 'to', 'come', 'to'] - [his] vs [his]\n",
      "Iter= 39000, Average Loss= 0.082623, Average Accuracy= 98.00%\n",
      "['but', 'this', 'time', 'the'] - [villagers] vs [villagers]\n",
      "Iter= 40000, Average Loss= 0.087641, Average Accuracy= 97.90%\n",
      "['actually', 'did', 'come', 'out'] - [from] vs [from]\n",
      "Iter= 41000, Average Loss= 0.085454, Average Accuracy= 97.80%\n",
      "['after', 'this', 'a', 'wolf'] - [actually] vs [actually]\n",
      "Iter= 42000, Average Loss= 0.091000, Average Accuracy= 98.10%\n",
      "['him', 'for', 'a', 'considerable'] - [time] vs [time]\n",
      "Iter= 43000, Average Loss= 0.080866, Average Accuracy= 98.30%\n",
      "['came', 'out', 'to', 'meet'] - [him] vs [him]\n",
      "Iter= 44000, Average Loss= 0.091073, Average Accuracy= 97.50%\n",
      "['him', 'all', 'day', ','] - [so] vs [so]\n",
      "Iter= 45000, Average Loss= 0.083651, Average Accuracy= 97.90%\n",
      "['there', 'was', 'once', 'a'] - [young] vs [young]\n",
      "Iter= 46000, Average Loss= 0.087412, Average Accuracy= 97.90%\n",
      "['man', 'of', 'the', 'village'] - [said] vs [said]\n",
      "Iter= 47000, Average Loss= 0.063918, Average Accuracy= 98.30%\n",
      "['the', 'wolf', 'made', 'a'] - [good] vs [good]\n",
      "Iter= 48000, Average Loss= 0.111500, Average Accuracy= 97.90%\n",
      "[',', 'who', 'had', 'been'] - [fooled] vs [fooled]\n",
      "Iter= 49000, Average Loss= 0.088438, Average Accuracy= 97.90%\n",
      "['a', 'wolf', 'actually', 'did'] - [come] vs [come]\n",
      "Iter= 50000, Average Loss= 0.099454, Average Accuracy= 97.40%\n",
      "['this', 'pleased', 'the', 'boy'] - [so] vs [so]\n",
      "End Of training Finished!\n",
      "time:  77.9815878868103\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and oint your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < epochs:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    " \n",
    "    ##############################################\n",
    "    \n",
    "    print(\"End Of training Finished!\")\n",
    "    print(\"time: \",time.time() - start_time)\n",
    "    print(\"For tensorboard visualisation run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"and oint your web browser to the returned link\")\n",
    "    ##############################################\n",
    "    # save your model \n",
    "    ##############################################\n",
    "    model_saver.save(session, model_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he could get a little\n",
      " Predicted word: little \n",
      " Word from text: little\n",
      "who tended his sheep at\n",
      " Predicted word: at \n",
      " Word from text: at\n",
      "village said : a liar\n",
      " Predicted word: liar \n",
      " Word from text: liar\n",
      "towards the village calling out\n",
      " Predicted word: out \n",
      " Word from text: out\n",
      "was once a young shepherd\n",
      " Predicted word: shepherd \n",
      " Word from text: shepherd\n",
      "mountain near a dark forest\n",
      " Predicted word: forest \n",
      " Word from text: forest\n",
      ". so the wolf made\n",
      " Predicted word: made \n",
      " Word from text: made\n",
      "wolf , still louder than\n",
      " Predicted word: than \n",
      " Word from text: than\n",
      "off the boy's flock ,\n",
      " Predicted word: , \n",
      " Word from text: ,\n",
      "a little company and some\n",
      " Predicted word: some \n",
      " Word from text: some\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "\n",
    "    sentences = [train_data[n:n + n_input + 1] for n in np.random.randint(0, len(train_data) - n_input - 1, 10)]\n",
    "    for sentence in sentences:\n",
    "        predicted = test(' '.join(sentence[:-1]), session, True)\n",
    "        print(' Predicted word:', predicted, '\\n Word from text:', sentence[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "As expected given the low accuracy obtained in the training phase, we do not have good guesses on the next words in this case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shortly after this a wolf actually did come out from the forest , and began to worry the sheep, and the boy of course cried out wolf , wolf , and the villagers came out to meet him , and some of them stopped with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the villagers came to to meet him , and some of them stopped with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the villagers came to to meet him , and some of them stopped with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the villagers came to to meet him , and some of them stopped with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the villagers came to to meet him , and some of them stopped with him for a considerable time .\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "    n = np.random.randint(0, len(train_data) - n_input)\n",
    "    words = train_data[n:n + n_input].tolist()\n",
    "    while words.count('.') < 5 and len(words) < 1000:  # set an upper bound on iterations\n",
    "        sentence = ' '.join(words[-n_input:])\n",
    "        words.append(test(sentence, session))\n",
    "\n",
    "if words[-1] != '.':\n",
    "    words.append('.')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" face=\"verdana\" > <i> \"There was once a young Shepherd Boy who tended his sheep at the foot of a mountain near a dark forest.\n",
    "\n",
    "It was rather lonely for him all day, so he thought upon a plan by which he could get a little company and some excitement.\n",
    "He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.\n",
    "This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n",
    "But shortly after this a Wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out \"Wolf, Wolf,\" still louder than before.\n",
    "But this time the villagers, who had been fooled twice before, thought the boy was again deceiving them, and nobody stirred to come to his help.\n",
    "So the Wolf made a good meal off the boy's flock, and when the boy complained, the wise man of the village said:\n",
    "\"A liar will not be believed, even when he speaks the truth.\"  \"</i> </font>.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "In this case, we can see how we are in a midway between n_input = 3 and n_input = 5. We do not obtain exactly the same text as the original one, but we do not get stuck in a loop. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "n_layers = 3\n",
    "model_path = 'lstm_model/n%d_%d' % (n_input, n_layers)\n",
    "\n",
    "# For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "# LSTM  weights and biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size]))}\n",
    "\n",
    "# build the model\n",
    "pred = lstm_model(x, weights, biases, n_hidden=n_hidden, n_input=n_input, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Loss/Cost and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Cross Entropy loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)  # use RMSProp Optimizer\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We give you here the Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run the cell\n",
    "def test(sentence, session, verbose=False):\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != n_input:\n",
    "        print(\"sentence length should be equal to\", n_input, \"!\")\n",
    "    try:\n",
    "        symbols_inputs = [dictionary[str(words[i - n_input])] for i in range(n_input)]\n",
    "        keys = np.reshape(np.array(symbols_inputs), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        words.append(reverse_dictionary[onehot_pred_index])\n",
    "        sentence = \" \".join(words)\n",
    "        if verbose:\n",
    "            print(sentence)\n",
    "        return reverse_dictionary[onehot_pred_index]\n",
    "    except:\n",
    "        print(\" \".join([\"Word\", words[i - n_input], \"not in dictionary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : LSTM Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Training process, at each epoch, 3 words are taken from the training data, encoded to integer to form the input vector. The training labels are one-hot vector encoding the word that comes after the 3 inputs words. Display the loss and the training accuracy every 1000 iteration. Save the model at the end of training in the **lstm_model** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Average Loss= 4.513784, Average Accuracy= 7.80%\n",
      "['not', 'be', 'believed'] - [,] vs [a]\n",
      "Iter= 2000, Average Loss= 3.925602, Average Accuracy= 9.10%\n",
      "['he', 'speaks', 'the'] - [truth] vs [boy]\n",
      "Iter= 3000, Average Loss= 3.067097, Average Accuracy= 25.90%\n",
      "['who', 'tended', 'his'] - [sheep] vs [.]\n",
      "Iter= 4000, Average Loss= 2.640464, Average Accuracy= 36.50%\n",
      "['the', 'foot', 'of'] - [a] vs [,]\n",
      "Iter= 5000, Average Loss= 2.246926, Average Accuracy= 45.00%\n",
      "['forest', '.', 'it'] - [was] vs [was]\n",
      "Iter= 6000, Average Loss= 2.146455, Average Accuracy= 43.70%\n",
      "['near', 'a', 'dark'] - [forest] vs [forest]\n",
      "Iter= 7000, Average Loss= 1.503713, Average Accuracy= 59.90%\n",
      "['near', 'a', 'dark'] - [forest] vs [forest]\n",
      "Iter= 8000, Average Loss= 1.542260, Average Accuracy= 58.50%\n",
      "['could', 'get', 'a'] - [little] vs [even]\n",
      "Iter= 9000, Average Loss= 1.001263, Average Accuracy= 72.80%\n",
      "['could', 'get', 'a'] - [little] vs [even]\n",
      "Iter= 10000, Average Loss= 1.069173, Average Accuracy= 70.20%\n",
      "['rather', 'lonely', 'for'] - [him] vs [him]\n",
      "Iter= 11000, Average Loss= 0.892328, Average Accuracy= 76.70%\n",
      "['it', 'was', 'rather'] - [lonely] vs [lonely]\n",
      "Iter= 12000, Average Loss= 0.805035, Average Accuracy= 79.50%\n",
      "['company', 'and', 'some'] - [excitement] vs [excitement]\n",
      "Iter= 13000, Average Loss= 0.923104, Average Accuracy= 74.20%\n",
      "['which', 'he', 'could'] - [get] vs [get]\n",
      "Iter= 14000, Average Loss= 0.622134, Average Accuracy= 82.00%\n",
      "['towards', 'the', 'village'] - [calling] vs [calling]\n",
      "Iter= 15000, Average Loss= 0.557123, Average Accuracy= 83.50%\n",
      "['and', 'the', 'villagers'] - [came] vs [of]\n",
      "Iter= 16000, Average Loss= 0.519005, Average Accuracy= 84.40%\n",
      "['the', 'village', 'calling'] - [out] vs [:]\n",
      "Iter= 17000, Average Loss= 0.547352, Average Accuracy= 84.50%\n",
      "['down', 'towards', 'the'] - [village] vs [little]\n",
      "Iter= 18000, Average Loss= 0.464414, Average Accuracy= 86.30%\n",
      "['wolf', ',', 'wolf'] - [,] vs [,]\n",
      "Iter= 19000, Average Loss= 0.421004, Average Accuracy= 86.10%\n",
      "['he', 'rushed', 'down'] - [towards] vs [towards]\n",
      "Iter= 20000, Average Loss= 0.432856, Average Accuracy= 86.70%\n",
      "['out', 'wolf', ','] - [wolf] vs [wolf]\n",
      "Iter= 21000, Average Loss= 0.348661, Average Accuracy= 89.70%\n",
      "[',', 'and', 'the'] - [villagers] vs [villagers]\n",
      "Iter= 22000, Average Loss= 0.385805, Average Accuracy= 87.90%\n",
      "['out', 'to', 'meet'] - [him] vs [him]\n",
      "Iter= 23000, Average Loss= 0.314475, Average Accuracy= 90.50%\n",
      "['calling', 'out', 'wolf'] - [,] vs [,]\n",
      "Iter= 24000, Average Loss= 0.322964, Average Accuracy= 89.20%\n",
      "['meet', 'him', ','] - [and] vs [and]\n",
      "Iter= 25000, Average Loss= 0.262786, Average Accuracy= 91.40%\n",
      "['with', 'him', 'for'] - [a] vs [a]\n",
      "Iter= 26000, Average Loss= 0.267740, Average Accuracy= 91.70%\n",
      "['.', 'this', 'pleased'] - [the] vs [the]\n",
      "Iter= 27000, Average Loss= 0.268132, Average Accuracy= 92.00%\n",
      "['them', 'stopped', 'with'] - [him] vs [him]\n",
      "Iter= 28000, Average Loss= 0.237582, Average Accuracy= 92.20%\n",
      "['a', 'considerable', 'time'] - [.] vs [.]\n",
      "Iter= 29000, Average Loss= 0.250948, Average Accuracy= 92.40%\n",
      "['the', 'boy', 'so'] - [much] vs [much]\n",
      "Iter= 30000, Average Loss= 0.303931, Average Accuracy= 90.80%\n",
      "['time', '.', 'this'] - [pleased] vs [pleased]\n",
      "Iter= 31000, Average Loss= 0.266663, Average Accuracy= 91.20%\n",
      "['trick', ',', 'and'] - [again] vs [when]\n",
      "Iter= 32000, Average Loss= 0.272254, Average Accuracy= 91.70%\n",
      "[',', 'and', 'again'] - [the] vs [the]\n",
      "Iter= 33000, Average Loss= 0.227739, Average Accuracy= 92.60%\n",
      "['but', 'shortly', 'after'] - [this] vs [this]\n",
      "Iter= 34000, Average Loss= 0.213474, Average Accuracy= 93.00%\n",
      "['out', 'from', 'the'] - [forest] vs [forest]\n",
      "Iter= 35000, Average Loss= 0.238117, Average Accuracy= 93.70%\n",
      "['forest', ',', 'and'] - [began] vs [began]\n",
      "Iter= 36000, Average Loss= 0.230955, Average Accuracy= 92.90%\n",
      "['and', 'the', 'boy'] - [of] vs [of]\n",
      "Iter= 37000, Average Loss= 0.220249, Average Accuracy= 93.80%\n",
      "['than', 'before', '.'] - [but] vs [but]\n",
      "Iter= 38000, Average Loss= 0.260863, Average Accuracy= 91.40%\n",
      "['villagers', ',', 'who'] - [had] vs [had]\n",
      "Iter= 39000, Average Loss= 0.245042, Average Accuracy= 91.80%\n",
      "['the', 'boy', 'was'] - [again] vs [course]\n",
      "Iter= 40000, Average Loss= 0.219123, Average Accuracy= 92.60%\n",
      "['who', 'had', 'been'] - [fooled] vs [fooled]\n",
      "Iter= 41000, Average Loss= 0.219423, Average Accuracy= 92.80%\n",
      "['and', 'nobody', 'stirred'] - [to] vs [to]\n",
      "Iter= 42000, Average Loss= 0.192120, Average Accuracy= 93.50%\n",
      "['wolf', 'made', 'a'] - [good] vs [good]\n",
      "Iter= 43000, Average Loss= 0.288994, Average Accuracy= 90.40%\n",
      "[\"boy's\", 'flock', ','] - [and] vs [and]\n",
      "Iter= 44000, Average Loss= 0.163745, Average Accuracy= 94.00%\n",
      "[',', 'the', 'wise'] - [man] vs [man]\n",
      "Iter= 45000, Average Loss= 0.261602, Average Accuracy= 92.50%\n",
      "['man', 'of', 'the'] - [village] vs [village]\n",
      "Iter= 46000, Average Loss= 0.157432, Average Accuracy= 94.50%\n",
      "['when', 'he', 'speaks'] - [the] vs [the]\n",
      "Iter= 47000, Average Loss= 0.274173, Average Accuracy= 91.10%\n",
      "['when', 'he', 'speaks'] - [the] vs [the]\n",
      "Iter= 48000, Average Loss= 0.244698, Average Accuracy= 92.60%\n",
      "['the', 'foot', 'of'] - [a] vs [a]\n",
      "Iter= 49000, Average Loss= 0.212566, Average Accuracy= 93.00%\n",
      "['shepherd', 'boy', 'who'] - [tended] vs [tended]\n",
      "Iter= 50000, Average Loss= 0.255610, Average Accuracy= 91.20%\n",
      "['was', 'once', 'a'] - [young] vs [young]\n",
      "End Of training Finished!\n",
      "time:  76.39229130744934\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and oint your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < epochs:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    " \n",
    "    ##############################################\n",
    "    \n",
    "    print(\"End Of training Finished!\")\n",
    "    print(\"time: \",time.time() - start_time)\n",
    "    print(\"For tensorboard visualisation run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"and oint your web browser to the returned link\")\n",
    "    ##############################################\n",
    "    # save your model \n",
    "    ##############################################\n",
    "    model_saver.save(session, model_path)\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4 : Test your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1. Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your model (using the model_saved variable given in the training session) and test the sentences :\n",
    "- 'get a little' \n",
    "- 'nobody tried to'\n",
    "- Try with other sentences using words from the story's vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a little company\n",
      "nobody tried to wolf\n",
      "\n",
      "\n",
      ", and some the\n",
      " Predicted word: the \n",
      " Word from text: of\n",
      "but this time the\n",
      " Predicted word: the \n",
      " Word from text: the\n",
      "this time the villagers\n",
      " Predicted word: villagers \n",
      " Word from text: villagers\n",
      "had been fooled twice\n",
      " Predicted word: twice \n",
      " Word from text: twice\n",
      "a liar will not\n",
      " Predicted word: not \n",
      " Word from text: not\n",
      "this a wolf actually\n",
      " Predicted word: actually \n",
      " Word from text: actually\n",
      "to his help .\n",
      " Predicted word: . \n",
      " Word from text: .\n",
      "boy so much that\n",
      " Predicted word: that \n",
      " Word from text: that\n",
      "young shepherd boy who\n",
      " Predicted word: who \n",
      " Word from text: who\n",
      "of the village said\n",
      " Predicted word: said \n",
      " Word from text: said\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "\n",
    "    sentences = ['get a little', 'nobody tried to']\n",
    "    for sentence in sentences:\n",
    "        test(sentence, session, True)\n",
    "    \n",
    "    print('\\n')\n",
    "    sentences = [train_data[n:n + n_input + 1] for n in np.random.randint(0, len(train_data) - n_input - 1, 10)]\n",
    "    for sentence in sentences:\n",
    "        predicted = test(' '.join(sentence[:-1]), session, True)\n",
    "        print(' Predicted word:', predicted, '\\n Word from text:', sentence[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. More fun with the Fable Writer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the RNN/LSTM model learned in the previous question to create a\n",
    "new story/fable.\n",
    "For this you will choose 3 words from the dictionary which will start your\n",
    "story and initialize your network. Using those 3 words the RNN will generate\n",
    "the next word or the story. Using the last 3 words (the newly predicted one\n",
    "and the last 2 from the input) you will use the network to predict the 5\n",
    "word of the story.. and so on until your story is 5 sentence long. \n",
    "Make a point at the end of your story. \n",
    "To implement that, you will use the test function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", the wise man of the village said out wolf , wolf , still louder than before . but this time the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and some the villagers came out to meet him , and .\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, model_path)\n",
    "    n = np.random.randint(0, len(train_data) - n_input)\n",
    "    words = train_data[n:n + n_input].tolist()\n",
    "    while words.count('.') < 5 and len(words) < 1000:  # set an upper bound on iterations\n",
    "        sentence = ' '.join(words[-n_input:])\n",
    "        words.append(test(sentence, session))\n",
    "\n",
    "if words[-1] != '.':\n",
    "    words.append('.')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
